{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import math\n",
        "\n",
        "import sys\n",
        "sys.setrecursionlimit(10 ** 5)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Downloading brown corpus\n",
        "import nltk \n",
        "nltk.download('brown')\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import webtext\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE, KneserNeyInterpolated"
      ],
      "metadata": {
        "id": "dPMzFcnbhOqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08505cde-36a7-4ed1-bcee-b58ea58ed865"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "jur8x7diGPLu"
      },
      "outputs": [],
      "source": [
        "def splitPairs(word):\n",
        "   return [(word[:i+1], word[i+1:]) for i in range(len(word))]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def segment(word):\n",
        "   if not word: return []\n",
        "   allSegmentations = [[first] + segment(rest)\n",
        "                       for (first, rest) in splitPairs(word)]\n",
        "   return max(allSegmentations, key = wordSeqFitness)"
      ],
      "metadata": {
        "id": "qi_3PgeMST_a"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OneGramDist(dict):\n",
        "   def __init__(self):\n",
        "      self.gramCount = 0\n",
        "      for line in open('one-grams.txt'):\n",
        "         (word, count) = line[:-1].split('\\t')\n",
        "         self[word] = int(count)\n",
        "         self.gramCount += self[word]\n",
        "\n",
        "   def __call__(self, word):\n",
        "      if word in self:\n",
        "         return float(self[word]) / self.gramCount\n",
        "      else:\n",
        "         return 1.0 / self.gramCount\n"
      ],
      "metadata": {
        "id": "zZ15WI5_SUGj"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "singleWordProb = OneGramDist()\n",
        "def wordSeqFitness(words):\n",
        "   return functools.reduce(lambda x,y: x+y,\n",
        "     (math.log10(singleWordProb(w)) for w in words))"
      ],
      "metadata": {
        "id": "rOWJX0DRSUI5"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment('hellosir')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCvb5StXhUbo",
        "outputId": "4ae6064a-be35-4c53-ef38-4a7cb9e838a3"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'sir']"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segment('howareyou')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyVOmNIOhXh3",
        "outputId": "7178f630-d5e2-4698-d204-72cbbb0bc3b0"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how', 'are', 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segment('himynameis')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHxHlElphiMg",
        "outputId": "25a3fcea-84df-4fe1-b251-2612d5d734ec"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['himynameis']"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improvement Version"
      ],
      "metadata": {
        "id": "JpVYPCO9h-Cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def splitPairs(word):\n",
        "   return [(word[:i+1], word[i+1:]) for i in range(len(word))]"
      ],
      "metadata": {
        "id": "iAO7R9hliCXU"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OneGramDist(dict):\n",
        "   def __init__(self):\n",
        "      self.gramCount = 0\n",
        "      for line in open('one-grams.txt'):\n",
        "         (word, count) = line[:-1].split('\\t')\n",
        "         self[word] = int(count)\n",
        "         self.gramCount += self[word]\n",
        "\n",
        "   def __call__(self, key):\n",
        "      if key in self:\n",
        "         return float(self[key]) / self.gramCount\n",
        "      else:\n",
        "         return 1.0 / (self.gramCount * 10**(len(key) - 2))"
      ],
      "metadata": {
        "id": "vUqtPHu_SULP"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "singleWordProb = OneGramDist()\n",
        "def wordSeqFitness(words):\n",
        "   return functools.reduce(lambda x,y: x+y,\n",
        "     (math.log10(singleWordProb(w)) for w in words))"
      ],
      "metadata": {
        "id": "6UsZ7MgKiNCv"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def memoize(f):\n",
        "   cache = {}\n",
        "\n",
        "   def memoizedFunction(*args):\n",
        "      if args not in cache:\n",
        "         cache[args] = f(*args)\n",
        "      return cache[args]\n",
        "\n",
        "   memoizedFunction.cache = cache\n",
        "   return memoizedFunction\n",
        "\n",
        "@memoize\n",
        "def segment(word):\n",
        "   if not word: return []\n",
        "   word = word.lower() # change to lower case\n",
        "   allSegmentations = [[first] + segment(rest) for (first,rest) in splitPairs(word)]\n",
        "   return max(allSegmentations, key = wordSeqFitness)\n",
        "\n",
        "def splitPairs(word, maxLen=20):\n",
        "   return [(word[:i+1], word[i+1:]) for i in range(max(len(word), maxLen))]\n",
        "\n",
        "@memoize\n",
        "def segmentWithProb(word):\n",
        "   segmented = segment(word)\n",
        "   return (wordSeqFitness(segmented), segmented)"
      ],
      "metadata": {
        "id": "nGkVcGbRYaND"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment('todayisanauspiciousday')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLN1BKFzhpCg",
        "outputId": "bd4ca09b-6309-4895-8783-20ce23c0d2cb"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['today', 'is', 'an', 'auspicious', 'day']"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segment('hewonthecricketmatch')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ry-ddYkjL4u",
        "outputId": "30ed61fc-7d3f-4c3f-e4a3-f18ae5608c2b"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'won', 'the', 'cricket', 'match']"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segment('Myunscientificfrienddoesnotbelievethathumanstatureismeasurableintermsofspeed')"
      ],
      "metadata": {
        "id": "ejaYCeORhyQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd57c81e-74ff-4ad9-d3a8-73749ecf7070"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my',\n",
              " 'unscientific',\n",
              " 'friend',\n",
              " 'does',\n",
              " 'not',\n",
              " 'believe',\n",
              " 'that',\n",
              " 'human',\n",
              " 'stature',\n",
              " 'is',\n",
              " 'measurable',\n",
              " 'in',\n",
              " 'terms',\n",
              " 'of',\n",
              " 'speed']"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segment('Myhostwentoverandstaredoutthewindowathispeacocks')"
      ],
      "metadata": {
        "id": "KH2gIqyqlOB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa025a8a-5a67-442e-87b9-b9487fd57432"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my',\n",
              " 'host',\n",
              " 'went',\n",
              " 'over',\n",
              " 'and',\n",
              " 'stared',\n",
              " 'out',\n",
              " 'the',\n",
              " 'window',\n",
              " 'a',\n",
              " 'this',\n",
              " 'peacocks']"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segment('Indiawoncricketmatchonauspiciousdayofhittingacenturyandafifty')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGMmRzzbbyVN",
        "outputId": "f8ebe8d9-9923-41e2-bfd1-7fd92eb597ba"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['india',\n",
              " 'won',\n",
              " 'cricket',\n",
              " 'match',\n",
              " 'on',\n",
              " 'auspicious',\n",
              " 'day',\n",
              " 'of',\n",
              " 'hitting',\n",
              " 'a',\n",
              " 'century',\n",
              " 'and',\n",
              " 'a',\n",
              " 'fifty']"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of all sentences present in \"brown\" coupus\n",
        "all_texts = nltk.corpus.brown.fileids()\n",
        "full_brown = []\n",
        "\n",
        "for text in all_texts:\n",
        "    para = nltk.corpus.brown.sents(text)\n",
        "    full_brown += [list(i) for i in para]\n",
        "\n",
        "print(\"Total number of sentences in Brown corpus :\", len(full_brown))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31l1LN1SaVxY",
        "outputId": "3406cfde-e2ff-4ee6-f2c0-3a6c4f02483b"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sentences in Brown corpus : 57340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(full_brown, test_size=0.2)"
      ],
      "metadata": {
        "id": "vDpwU0lqabiQ"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n4CHa8gbPO1",
        "outputId": "739237a8-8fb5-4fc9-9d5c-e3957f022a5a"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Hence', '**zg', 'must', 'have', 'either', 'a', 'regulus', 'of', 'Af-fold', 'secants', 'or', 'a', 'regulus', 'of', 'Af-fold', 'secants', '.'], ['It', 'was', 'small', 'comfort', 'for', 'insomnia', '.'], ['Whether', 'he', 'sang', 'well', 'or', 'badly', 'had', 'nothing', 'to', 'do', 'with', 'it', '.'], ['A', 'trained', 'marksman', 'shooting', 'five', 'rounds', 'at', 'a', 'target', ',', 'all', 'under', 'practically', 'the', 'same', 'conditions', ',', 'may', 'hit', 'the', \"bull's-eye\", 'from', '0', 'to', '5', 'times', '.'], ['Yes', '!', '!'], ['Softer', 'knives', 'would', 'blunt', 'very', 'rapidly', ',', 'making', 'the', 'value', 'of', 'Af', 'inexact', '.'], ['A', '``', 'cattaloe', \"''\", 'was', 'a', 'hybrid', 'offspring', 'of', 'buffalo', 'and', 'cattle', '.'], ['No', 'more', '.'], ['``', 'Those', 'damn', 'punks', '--', 'taking', 'work', 'away', 'from', 'men', 'who', 'need', 'it', \"''\", '.'], ['Backs', 'higher', 'fees']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCaV1sHZbSrm",
        "outputId": "6eacc4e7-ad41-45c1-8908-7467c65b4e23"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11468"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sentences):\n",
        "  punc = r\"\"\"!()-[]{};:'\"\\, ``<>./?@#$%^&*_~\"\"\"\n",
        "  read = \" \".join(sentences)\n",
        "  for ele in read:\n",
        "      if ele in punc:\n",
        "          read = read.replace(ele, \" \") \n",
        "  read = read.lower()\n",
        "\n",
        "  # This will convert the word into tokens\n",
        "  text_tokens = word_tokenize(read)\n",
        "\n",
        "  # Remove all the stopwords from the tokens\n",
        "  tokens_without_sw = [\n",
        "        word for word in text_tokens if not word in stopwords.words(\"english\")\n",
        "    ]\n",
        "  # Initialize the stemmer\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  # Stem all the words\n",
        "  tokens_without_sw_stem = [ps.stem(word) for word in tokens_without_sw]  \n",
        "\n",
        "  pre_text = [i for i in tokens_without_sw_stem if not i.isnumeric()]\n",
        "  return pre_text"
      ],
      "metadata": {
        "id": "RiT97LzLbhW8"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ptrain = [preprocess(i) for i in train]\n",
        "ptest = [preprocess(i) for i in test]"
      ],
      "metadata": {
        "id": "F02R_-yKjRxm"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "train_data, padded_vocab = padded_everygram_pipeline(n, ptrain)\n",
        "lm_model = MLE(n)\n",
        "lm_model.fit(train_data, padded_vocab)\n",
        "print(lm_model.counts)\n",
        "test_data, _ = padded_everygram_pipeline(n, ptest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZvm1z_UjdMQ",
        "outputId": "42f6c8f5-6dba-4dcd-f8b6-fb72ffe4c326"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<NgramCounter with 3 ngram orders and 1693218 ngrams>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = 0\n",
        "co = 0\n",
        "for i in ptest:  \n",
        "  co += 1\n",
        "  if not i:\n",
        "    continue\n",
        "  sent = ''.join(i)\n",
        "  try:\n",
        "    seg = segment(sent)\n",
        "  except:\n",
        "    print(co, i)\n",
        "    pass\n",
        "  acc += sum([1 for j in seg if j in i]) / len(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOWvp_CxcgUl",
        "outputId": "4d04bd6d-aa90-4114-94b3-68c45142c65f"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "267 ['white', 'colonnad', 'cedar', 'roof', 'southern', 'mansion', 'directli', 'traceabl', 'via', 'grey', 'buff', 'stone', 'grey', 'ski', 'england', 'golden', 'stucco', 'one', 'particular', 'part', 'blue', 'south', 'palladian', 'orbit', 'stretch', 'vicenza', 'old', 'mind', 'andrea', 'palladio', 'still', 'smile', 'behind', 'mani', 'old', 'rock', 'chair', 'southern', 'porch', 'deep', 'friez', 'architecton', 'music', 'rise', 'firm', 'shallow', 'freez', 'kitchen', 'feel', 'light', 'shade', 'bring', 'glitter', 'tall', 'mint', 'julep', 'sens', 'column', 'frame', 'warm', 'velvet', 'night', 'brought', 'togeth', 'million', 'coupl', 'mate', 'lip']\n",
            "2155 ['suck', 'breath', 'kept', 'quiet', 'killpath', 'laid', 'sheet', 'wound', 'gold', 'wire', 'stem', 'glass', 'around', 'ear', 'eye', 'report', 'lay', 'desk', 'inton', 'act', 'lieuten', 'gunnar', 'matson', 'one', 'fail', 'see', 'station', 'keeper', 'properli', 'reliev', 'two', 'absent', 'throughout', 'entir', 'watch', 'without', 'check', 'station', 'activ', 'whereabout', 'section', 'sergeant', 'three', 'permit', 'member', 'homicid', 'detail', 'inspector', 'bureau', 'arrog', 'conveni', 'patrolman', 'therebi', 'prevent', 'carri', 'proper', 'assign', 'four', 'fail', 'notifi', 'station', 'command', 'act', 'captain', 'killpath', 'homicid', 'occur', 'district', 'five', 'frequent', 'extraleg', 'establish', 'known', 'hour', 'spot', 'purpos', 'unoffici', 'purportedli', 'social', 'natur', 'six', 'lean', 'back', 'peel', 'glass', 'fail', 'co', 'oper', 'act', 'captain', 'return', 'promptli', 'order']\n",
            "4645 ['deposit', 'rupe', 'account', 'govern', 'unit', 'state', 'america', 'payment', 'commod', 'ocean', 'transport', 'cost', 'financ', 'govern', 'unit', 'state', 'america', 'except', 'excess', 'cost', 'result', 'requir', 'unit', 'state', 'flag', 'vessel', 'use', 'shall', 'made', 'rate', 'exchang', 'unit', 'state', 'dollar', 'gener', 'applic', 'import', 'transact', 'exclud', 'import', 'grant', 'preferenti', 'rate', 'effect', 'date', 'dollar', 'disburs', 'unit', 'state', 'bank', 'govern', 'unit', 'state', 'america', 'provid', 'purchas', 'author']\n",
            "10617 ['behind', 'charli', 'chaplin', 'moustach', 'truant', 'lock', 'hair', 'alway', 'cover', 'forehead', 'behind', 'tirad', 'sulki', 'silenc', 'passion', 'orat', 'occasion', 'dull', 'evas', 'stare', 'behind', 'prejudic', 'cynic', 'total', 'amor', 'behavior', 'behind', 'even', 'tendenc', 'great', 'strateg', 'mistak', 'lay', 'statesman', 'mean', 'qualiti', 'shrewd', 'calcul', 'mani', 'way', 'realist', 'endow', 'like', 'stalin', 'consider', 'power', 'dissimul', 'capabl', 'play', 'card', 'close', 'chest', 'desir', 'yet', 'bold', 'resolut', 'decis', 'possess', 'one', 'gift', 'stalin', 'possess', 'abil', 'rous', 'men', 'fever', 'pitch', 'person', 'devot', 'enthusiasm', 'power', 'spoken', 'word']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accp = acc/len(test)\n",
        "print(\"Accuracy of the Language Model:\", accp*100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgcXBLl9fgbw",
        "outputId": "afd4f4b6-f789-4ee4-a959-347542364805"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Language Model: 74.97050729996356 %\n"
          ]
        }
      ]
    }
  ]
}